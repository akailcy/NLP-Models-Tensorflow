{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train-test.json') as fopen:\n",
    "    dataset = json.load(fopen)\n",
    "    \n",
    "with open('dictionary.json') as fopen:\n",
    "    dictionary = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = dataset['train_X']\n",
    "train_Y = dataset['train_Y']\n",
    "test_X = dataset['test_X']\n",
    "test_Y = dataset['test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['from', 'to'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_from = dictionary['from']['dictionary']\n",
    "rev_dictionary_from = dictionary['from']['rev_dictionary']\n",
    "\n",
    "dictionary_to = dictionary['to']['dictionary']\n",
    "rev_dictionary_to = dictionary['to']['rev_dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rachel Pike : The science behind a climate headline EOS'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(train_X)):\n",
    "    train_X[i] += ' EOS'\n",
    "    \n",
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I speak in <NUM> minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four - year - old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than <NUM> years ago , bonds that took hold in the life of that small girl and never let go - - that small girl now living in San Francisco and speaking to you today ? EOS'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(test_X)):\n",
    "    test_X[i] += ' EOS'\n",
    "    \n",
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_second_dim(x, desired_size):\n",
    "    padding = tf.tile([[[0.0]]], tf.stack([tf.shape(x)[0], desired_size - tf.shape(x)[1], tf.shape(x)[2]], 0))\n",
    "    return tf.concat([x, padding], 1)\n",
    "\n",
    "def ln(inputs, epsilon = 1e-8, scope=\"ln\"):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V,\n",
    "                                 causality=False, dropout_rate=0.,\n",
    "                                 training=True,\n",
    "                                 scope=\"scaled_dot_product_attention\"):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        d_k = Q.get_shape().as_list()[-1]\n",
    "\n",
    "        outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n",
    "        outputs /= d_k ** 0.5\n",
    "        outputs = mask(outputs, Q, K, type=\"key\")\n",
    "        if causality:\n",
    "            outputs = mask(outputs, type=\"future\")\n",
    "        outputs = tf.nn.softmax(outputs)\n",
    "        attention = tf.transpose(outputs, [0, 2, 1])\n",
    "        #tf.summary.image(\"attention\", tf.expand_dims(attention[:1], -1))\n",
    "        outputs = mask(outputs, Q, K, type=\"query\")\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=training)\n",
    "        outputs = tf.matmul(outputs, V)\n",
    "    return outputs\n",
    "\n",
    "def mask(inputs, queries=None, keys=None, type=None):\n",
    "    padding_num = -2 ** 32 + 1\n",
    "    if type in (\"k\", \"key\", \"keys\"):\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1))  # (N, T_k)\n",
    "        masks = tf.expand_dims(masks, 1) # (N, 1, T_k)\n",
    "        masks = tf.tile(masks, [1, tf.shape(queries)[1], 1])  # (N, T_q, T_k)\n",
    "        paddings = tf.ones_like(inputs) * padding_num\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)  # (N, T_q, T_k)\n",
    "    elif type in (\"q\", \"query\", \"queries\"):\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1))  # (N, T_q)\n",
    "        masks = tf.expand_dims(masks, -1)  # (N, T_q, 1)\n",
    "        masks = tf.tile(masks, [1, 1, tf.shape(keys)[1]])  # (N, T_q, T_k)\n",
    "        outputs = inputs*masks\n",
    "    elif type in (\"f\", \"future\", \"right\"):\n",
    "        diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n",
    "        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)\n",
    "        paddings = tf.ones_like(masks) * padding_num\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)\n",
    "    else:\n",
    "        print(\"Check if you entered type correctly!\")\n",
    "\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def multihead_attention(queries, keys, values,\n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\"):\n",
    "    d_model = queries.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, d_model, use_bias=False) # (N, T_q, d_model)\n",
    "        K = tf.layers.dense(keys, d_model, use_bias=False) # (N, T_k, d_model)\n",
    "        V = tf.layers.dense(values, d_model, use_bias=False) # (N, T_k, d_model)\n",
    "        \n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
    "\n",
    "        outputs = scaled_dot_product_attention(Q_, K_, V_, causality, dropout_rate, training)\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n",
    "        outputs += queries\n",
    "        outputs = ln(outputs)\n",
    " \n",
    "    return outputs\n",
    "\n",
    "def ff(inputs, num_units, scope=\"positionwise_feedforward\"):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)\n",
    "        outputs = tf.layers.dense(outputs, num_units[1])\n",
    "        outputs += inputs\n",
    "        outputs = ln(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    V = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / V)\n",
    "\n",
    "def sinusoidal_position_encoding(inputs, mask, repr_dim):\n",
    "    T = tf.shape(inputs)[1]\n",
    "    pos = tf.reshape(tf.range(0.0, tf.to_float(T), dtype=tf.float32), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1]) * tf.expand_dims(tf.to_float(mask), -1)\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self, from_dict_size, to_dict_size, size_layer, learning_rate,\n",
    "                num_blocks = 4, num_heads = 8, ratio_hidden = 4, beam_width = 5):\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "        \n",
    "        encoder_embedding = tf.Variable(tf.random_uniform([from_dict_size, size_layer], -1, 1))\n",
    "        decoder_embedding = tf.Variable(tf.random_uniform([to_dict_size, size_layer], -1, 1))\n",
    "        \n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        \n",
    "        def forward(x, y, reuse = False):\n",
    "            encoder_embedded = tf.nn.embedding_lookup(encoder_embedding, x)\n",
    "            en_masks = tf.sign(x)\n",
    "            encoder_embedded += sinusoidal_position_encoding(x, en_masks, size_layer)\n",
    "            \n",
    "            for i in range(num_blocks):\n",
    "                with tf.variable_scope('encoder_self_attn_%d'%i,reuse=reuse):\n",
    "                    enc = multihead_attention(queries=encoder_embedded,\n",
    "                                              keys=encoder_embedded,\n",
    "                                              values=encoder_embedded,\n",
    "                                              num_heads=num_heads,\n",
    "                                              causality=False)\n",
    "                    enc = ff(enc, num_units=[size_layer * ratio_hidden, size_layer])\n",
    "            memory = enc\n",
    "            \n",
    "            decoder_embedded = tf.nn.embedding_lookup(decoder_embedding, y)\n",
    "            de_masks = tf.sign(y)\n",
    "            decoder_embedded += sinusoidal_position_encoding(y, de_masks, size_layer)\n",
    "            dec = decoder_embedded\n",
    "            \n",
    "            for i in range(num_blocks):\n",
    "                with tf.variable_scope('decoder_self_attn_%d'%i,reuse=reuse):\n",
    "                    dec = multihead_attention(queries=dec,\n",
    "                                              keys=dec,\n",
    "                                              values=dec,\n",
    "                                              num_heads=num_heads,\n",
    "                                              causality=True,\n",
    "                                              scope=\"self_attention\")\n",
    "\n",
    "                    dec = multihead_attention(queries=dec,\n",
    "                                              keys=memory,\n",
    "                                              values=memory,\n",
    "                                              num_heads=num_heads,\n",
    "                                              causality=False,\n",
    "                                              scope=\"vanilla_attention\")\n",
    "                    \n",
    "                    dec = ff(dec, num_units=[size_layer * ratio_hidden, size_layer])\n",
    "            \n",
    "            weights = tf.transpose(decoder_embedding)\n",
    "            logits = tf.einsum('ntd,dk->ntk', dec, weights)\n",
    "            return logits\n",
    "        \n",
    "        self.training_logits = forward(self.X, decoder_input)\n",
    "\n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        y_t = tf.argmax(self.training_logits,axis=2)\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.Y, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        initial_ids = tf.fill([batch_size], GO)\n",
    "        \n",
    "        def symbols_to_logits(ids):\n",
    "            x = tf.contrib.seq2seq.tile_batch(self.X, beam_width)\n",
    "            logits = forward(x, ids, reuse = True)\n",
    "            return logits[:, tf.shape(ids)[1]-1, :]\n",
    "        \n",
    "        final_ids, final_probs, _ = beam_search.beam_search(\n",
    "            symbols_to_logits,\n",
    "            initial_ids,\n",
    "            beam_width,\n",
    "            tf.reduce_max(self.X_seq_len),\n",
    "            to_dict_size,\n",
    "            0.0,\n",
    "            eos_id = EOS)\n",
    "        \n",
    "        self.predicting_ids = final_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 512\n",
    "learning_rate = 1e-4\n",
    "batch_size = 96\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n",
      "WARNING:tensorflow:From <ipython-input-10-474210a7070d>:102: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-474210a7070d>:72: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-10-474210a7070d>:45: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From <ipython-input-10-474210a7070d>:34: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Translator(len(dictionary_from), len(dictionary_to), size_layer, learning_rate)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i.split():\n",
    "            ints.append(dic.get(k,UNK))\n",
    "        X.append(ints)\n",
    "    return X\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = str_idx(train_X, dictionary_from)\n",
    "test_X = str_idx(test_X, dictionary_from)\n",
    "train_Y = str_idx(train_Y, dictionary_to)\n",
    "test_Y = str_idx(test_Y, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(model.predicting_ids, feed_dict = {model.X: [train_X[0]]}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:30<00:00,  4.20it/s, accuracy=0.153, cost=6.59] \n",
      "minibatch loop: 100%|██████████| 30/30 [00:03<00:00,  9.85it/s, accuracy=0.158, cost=6.6] \n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:21,  5.31it/s, accuracy=0.195, cost=5.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training avg loss 8.955545, training avg acc 0.108270\n",
      "epoch 1, testing avg loss 5.840093, testing avg acc 0.185351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:28<00:00,  4.23it/s, accuracy=0.21, cost=5.53] \n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.12it/s, accuracy=0.22, cost=5.7]  \n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:26,  5.21it/s, accuracy=0.285, cost=4.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training avg loss 5.173765, training avg acc 0.243477\n",
      "epoch 2, testing avg loss 4.984414, testing avg acc 0.264366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:29<00:00,  4.21it/s, accuracy=0.279, cost=4.91]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 10.81it/s, accuracy=0.255, cost=5.25]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:27,  5.19it/s, accuracy=0.325, cost=4.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training avg loss 4.533984, training avg acc 0.306413\n",
      "epoch 3, testing avg loss 4.555390, testing avg acc 0.311855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:20<00:00,  4.34it/s, accuracy=0.317, cost=4.47]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 10.90it/s, accuracy=0.292, cost=4.98]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:26,  5.22it/s, accuracy=0.365, cost=3.98]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training avg loss 4.126750, training avg acc 0.349728\n",
      "epoch 4, testing avg loss 4.273607, testing avg acc 0.342725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:19<00:00,  4.34it/s, accuracy=0.361, cost=4.04]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.08it/s, accuracy=0.306, cost=4.79]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:31,  5.11it/s, accuracy=0.398, cost=3.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training avg loss 3.816826, training avg acc 0.383024\n",
      "epoch 5, testing avg loss 4.091312, testing avg acc 0.360991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:18<00:00,  4.36it/s, accuracy=0.402, cost=3.65]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.12it/s, accuracy=0.314, cost=4.68]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:22,  5.30it/s, accuracy=0.426, cost=3.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training avg loss 3.562186, training avg acc 0.410961\n",
      "epoch 6, testing avg loss 3.968070, testing avg acc 0.373758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:18<00:00,  4.36it/s, accuracy=0.457, cost=3.24]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 10.96it/s, accuracy=0.315, cost=4.65]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:17,  5.40it/s, accuracy=0.458, cost=3.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training avg loss 3.342103, training avg acc 0.435845\n",
      "epoch 7, testing avg loss 3.900058, testing avg acc 0.381138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:18<00:00,  4.36it/s, accuracy=0.498, cost=2.87]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.12it/s, accuracy=0.33, cost=4.67] \n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:21,  5.32it/s, accuracy=0.483, cost=3.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training avg loss 3.146801, training avg acc 0.459015\n",
      "epoch 8, testing avg loss 3.896288, testing avg acc 0.386253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:18<00:00,  4.36it/s, accuracy=0.545, cost=2.49]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.08it/s, accuracy=0.326, cost=4.77]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:26,  5.21it/s, accuracy=0.498, cost=2.93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training avg loss 2.972501, training avg acc 0.480463\n",
      "epoch 9, testing avg loss 3.959981, testing avg acc 0.386002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:18<00:00,  4.36it/s, accuracy=0.6, cost=2.12]  \n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.06it/s, accuracy=0.319, cost=4.91]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:29,  5.16it/s, accuracy=0.52, cost=2.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training avg loss 2.818905, training avg acc 0.499701\n",
      "epoch 10, testing avg loss 4.049783, testing avg acc 0.387150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:18<00:00,  4.36it/s, accuracy=0.661, cost=1.82]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.10it/s, accuracy=0.32, cost=5.02] \n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:20,  5.33it/s, accuracy=0.533, cost=2.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, training avg loss 2.685967, training avg acc 0.515953\n",
      "epoch 11, testing avg loss 4.098845, testing avg acc 0.388923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:18<00:00,  4.36it/s, accuracy=0.69, cost=1.6]  \n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.15it/s, accuracy=0.316, cost=4.91]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:23,  5.27it/s, accuracy=0.556, cost=2.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, training avg loss 2.567834, training avg acc 0.530294\n",
      "epoch 12, testing avg loss 4.018679, testing avg acc 0.396831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:18<00:00,  4.36it/s, accuracy=0.737, cost=1.37]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.07it/s, accuracy=0.325, cost=4.85]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:20,  5.32it/s, accuracy=0.574, cost=2.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, training avg loss 2.453018, training avg acc 0.545141\n",
      "epoch 13, testing avg loss 4.005711, testing avg acc 0.397211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:19<00:00,  4.34it/s, accuracy=0.785, cost=1.16]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.07it/s, accuracy=0.313, cost=4.84]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:22,  5.29it/s, accuracy=0.6, cost=2.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, training avg loss 2.338308, training avg acc 0.560455\n",
      "epoch 14, testing avg loss 4.023633, testing avg acc 0.392258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:20<00:00,  4.33it/s, accuracy=0.802, cost=0.994]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.01it/s, accuracy=0.314, cost=4.88]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:22,  5.29it/s, accuracy=0.614, cost=2.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, training avg loss 2.229293, training avg acc 0.575232\n",
      "epoch 15, testing avg loss 4.066958, testing avg acc 0.388447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:21<00:00,  4.32it/s, accuracy=0.84, cost=0.838]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 10.98it/s, accuracy=0.299, cost=5.04]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:18,  5.36it/s, accuracy=0.617, cost=1.93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, training avg loss 2.127022, training avg acc 0.589416\n",
      "epoch 16, testing avg loss 4.188903, testing avg acc 0.383200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:21<00:00,  4.32it/s, accuracy=0.855, cost=0.728]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 10.96it/s, accuracy=0.311, cost=5.25]\n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:23,  5.26it/s, accuracy=0.631, cost=1.83]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, training avg loss 2.032683, training avg acc 0.602592\n",
      "epoch 17, testing avg loss 4.360122, testing avg acc 0.378222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:21<00:00,  4.33it/s, accuracy=0.882, cost=0.598]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 10.99it/s, accuracy=0.308, cost=5.5] \n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:27,  5.19it/s, accuracy=0.646, cost=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, training avg loss 1.948235, training avg acc 0.613778\n",
      "epoch 18, testing avg loss 4.509265, testing avg acc 0.375841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:21<00:00,  4.32it/s, accuracy=0.911, cost=0.493]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 10.99it/s, accuracy=0.305, cost=5.6] \n",
      "minibatch loop:   0%|          | 1/1389 [00:00<04:24,  5.24it/s, accuracy=0.671, cost=1.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, training avg loss 1.869872, training avg acc 0.624181\n",
      "epoch 19, testing avg loss 4.605663, testing avg acc 0.377575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1389/1389 [05:21<00:00,  4.33it/s, accuracy=0.939, cost=0.417]\n",
      "minibatch loop: 100%|██████████| 30/30 [00:02<00:00, 11.06it/s, accuracy=0.301, cost=5.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, training avg loss 1.785753, training avg acc 0.636835\n",
      "epoch 20, testing avg loss 4.688456, testing avg acc 0.378041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for e in range(epoch):\n",
    "    pbar = tqdm.tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'minibatch loop')\n",
    "    train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, len(train_X))\n",
    "        maxlen = max([len(s) for s in train_X[i : index] + train_Y[i : index]])\n",
    "        batch_x, seq_x = pad_sentence_batch(train_X[i : index], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(train_Y[i : index], PAD)\n",
    "        feed = {model.X: batch_x,\n",
    "                model.Y: batch_y}\n",
    "        accuracy, loss, _ = sess.run([model.accuracy,model.cost,model.optimizer],\n",
    "                                    feed_dict = feed)\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(accuracy)\n",
    "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
    "    \n",
    "    \n",
    "    pbar = tqdm.tqdm(\n",
    "        range(0, len(test_X), batch_size), desc = 'minibatch loop')\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, len(test_X))\n",
    "        batch_x, seq_x = pad_sentence_batch(test_X[i : index], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(test_Y[i : index], PAD)\n",
    "        feed = {model.X: batch_x,\n",
    "                model.Y: batch_y,}\n",
    "        accuracy, loss = sess.run([model.accuracy,model.cost],\n",
    "                                    feed_dict = feed)\n",
    "\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(accuracy)\n",
    "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
    "    \n",
    "    print('epoch %d, training avg loss %f, training avg acc %f'%(e+1,\n",
    "                                                                 np.mean(train_loss),np.mean(train_acc)))\n",
    "    print('epoch %d, testing avg loss %f, testing avg acc %f'%(e+1,\n",
    "                                                              np.mean(test_loss),np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_dictionary_to = {int(k): v for k, v in rev_dictionary_to.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 20\n",
    "\n",
    "batch_x, seq_x = pad_sentence_batch(test_X[: test_size], PAD)\n",
    "batch_y, seq_y = pad_sentence_batch(test_Y[: test_size], PAD)\n",
    "feed = {model.X: batch_x}\n",
    "logits = sess.run(model.predicting_ids, feed_dict = feed)[:,0,:]\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 predict: Làm cách nào tôi nói trong <NUM> phút ở đây , về ba thế hệ phụ nữ , về thế hệ sức mạnh của đời sống bằng cách hỗ trợ cuộc sống thực của bà bà ta hơn <NUM> năm sau khi bà bà bà bà bà bà còn nhỏ nhất hơn so với bà bà bà bà ấy hơn <NUM> năm trước hơn so với hơn so với hơn so với hơn so với hơn <NUM> năm so với hơn so với hơn so với hơn so với hơn <NUM> năm chúng ta ? Trong <NUM> câu trả\n",
      "0 actual: Làm sao tôi có thể trình bày trong <NUM> phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn <NUM> năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi - - cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "\n",
      "1 predict: Đây không phải là câu chuyện hoàn thiện . Câu chuyện của chúng ta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . đó là một câu chuyện . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "1 actual: Câu chuyện này chưa kết thúc .\n",
      "\n",
      "2 predict: Nó là một ứng dụng chúng vẫn đang được hợp lại với nhau . Hãy cùng làm việc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "2 actual: Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "\n",
      "3 predict: Để tôi kể cho các bạn vài mảnh giấy phiêu lưu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "3 actual: Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "\n",
      "4 predict: Hãy hình dung : một bong bóng gà đầu tiên của anh ta . Trong công việc của anh ấy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "4 actual: Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "\n",
      "5 predict: Ông ấy là một nhà thơ , một người đàn ông , một người duy nhất đã bị bỏ rơi trên đất nước và sự tự do ngoại trừ sự tự do ý chí . Đó là sự tự do và sự tự do ngôn ngữ và sự tự do . Nó chính là sự tự do . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5 actual: Ông là nhà thơ , nhà viết kịch , một người mà cả cuộc đời chênh vênh trên tia hi vọng duy nhất rằng đất nước ông sẽ độc lập tự do .\n",
      "\n",
      "6 predict: Hãy tưởng tượng anh ấy là nơi trên thực tế , đã hoàn toàn xấu xa của anh ta . Thật sự lãng phí . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . thực sự là một sự lãng lãng cốt . . . . . . .\n",
      "6 actual: Hãy tưởng tượng ông , một người cộng sản tiến vào , đối diện sự thật rằng cả cuộc đời ông đã phí hoài .\n",
      "\n",
      "7 predict: Từ lúc này , cho hàng loạt những người bạn đâu đó vì ông ấy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7 actual: Ngôn từ , qua bao năm tháng là bạn đồng hành với ông , giờ quay ra chế giễu ông .\n",
      "\n",
      "8 predict: Anh ấy đi vào sự tĩnh lặng . Thật ra ngoài . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "8 actual: Ông rút lui vào yên lặng .\n",
      "\n",
      "9 predict: Anh ta chết đuối nháy . Anh đã chết lịch sử . Anh ấy đã chết . Anh ấy đã chết . Anh ấy đã chết . Anh ấy đã chết lịch sử . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "9 actual: Ông qua đời , bị lịch sử quật ngã .\n",
      "\n",
      "10 predict: Anh ấy là ông nội tại ông nội . Anh ấy là ông ấy . Anh ấy đang thực hiện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "10 actual: Ông là ông của tôi .\n",
      "\n",
      "11 predict: Tôi không bao giờ biết nó trong cuộc sống thực sự . . . . . . . . . . . . . . . . . . . . . tôi không bao giờ biết thật sự . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "11 actual: Tôi chưa bao giờ gặp ông ngoài đời .\n",
      "\n",
      "12 predict: Nhưng cuộc sống của chúng ta hơn nhiều so với ký ức của chúng ta . Thừa đến hơn là ký ức . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . cuộc cuộc sống của chúng ta hơn nhiều thời điểm . . . . . . . . . . . . . . . .\n",
      "12 actual: Nhưng cuộc đời ta nhiều hơn những gì ta lưu trong kí ức nhiều .\n",
      "\n",
      "13 predict: Người bà tôi không bao giờ quên đời ông ấy . Hãy sửa cuộc sống của ông ta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . thì hãy hãy để tôi đang quên . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "13 actual: Bà tôi chưa bao giờ cho phép tôi quên cuộc đời của ông .\n",
      "\n",
      "14 predict: Bảo tôi không cho phép điều đó , bài học của tôi được kiểm tra tấn , và vâng , vâng , vâng , vâng , vâng , chúng ta . \" chúng ta có thể tiếp tục . . \" nhưng chúng ta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "14 actual: Nhiệm vụ của tôi là không để cuộc đời ấy qua trong vô vọng , và bài học của tôi là nhận ra rằng , vâng , lịch sử đã cố quật ngã chúng tôi , nhưng chúng tôi đã chịu đựng được .\n",
      "\n",
      "15 predict: Sau điểm màu tiếp theo là sự biến đổi trong một cái tổ chức nào đó buổi diễn chú . Hãy sửa nó . Hãy bận tâm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "15 actual: Mảnh ghép tiếp theo của tấm hình là một con thuyền trong sớm hoàng hôn lặng lẽ trườn ra biển .\n",
      "\n",
      "16 predict: Mẹ tôi , thật hạn , lúc <NUM> tuổi khi cô ấy mất hai cuộc hôn nhân của mình trong <NUM> cuộc hôn nhân hai cô bé nhỏ , với hai em gái nhỏ rồi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "16 actual: Mẹ tôi , Mai , mới <NUM> tuổi khi ba của mẹ mất - - đã lập gia đình , một cuộc hôn nhân sắp đặt trước , đã có hai cô con gái nhỏ .\n",
      "\n",
      "17 predict: Đối với cô ấy , sự sống của chính nó có một nhiệm vụ : Một gia đình và gia đình mới trong cuộc sống gia đình bà ấy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "17 actual: Với mẹ , cuộc đời cô đọng vào nhiệm vụ duy nhất : để gia đình mẹ trốn thoát và bắt đầu cuộc sống mới ở Úc .\n",
      "\n",
      "18 predict: Nó có thể khiến cô ấy không thành công . Nó sẽ không thành công . . . . . . . . . . . . . cô ấy không thể hiểu biết cô ấy sẽ đi theo . . . . . . . . . . . . . cô ấy có thể sẽ thành công . . . . . . . . . . . . . . . . . . . cô ấy có thể thành công việc . . . . . . . . . . . . . .\n",
      "18 actual: Mẹ không bao giờ chấp nhận được là mẹ sẽ không thành công .\n",
      "\n",
      "19 predict: Một năm sau - bốt tái sinh ra <NUM> ma trận ngoại bào sắt ra đại dương như một con cá voi như một chiếc Syria . Những đánh cá voi đánh cá mồi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "19 actual: Thế là sau bốn năm , một trường thiên đằng đẵng hơn cả trong truyện , một chiếc thuyền trườn ra biển nguỵ trang là thuyền đánh cá .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rejected = ['PAD', 'EOS', 'UNK', 'GO']\n",
    "\n",
    "for i in range(test_size):\n",
    "    predict = [rev_dictionary_to[i] for i in logits[i] if rev_dictionary_to[i] not in rejected]\n",
    "    actual = [rev_dictionary_to[i] for i in batch_y[i] if rev_dictionary_to[i] not in rejected]\n",
    "    print(i, 'predict:', ' '.join(predict))\n",
    "    print(i, 'actual:', ' '.join(actual))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
