{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.25.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train-test.json') as fopen:\n",
    "    dataset = json.load(fopen)\n",
    "    \n",
    "with open('dictionary.json') as fopen:\n",
    "    dictionary = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = dataset['train_X']\n",
    "train_Y = dataset['train_Y']\n",
    "test_X = dataset['test_X']\n",
    "test_Y = dataset['test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['from', 'to'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_from = dictionary['from']['dictionary']\n",
    "rev_dictionary_from = dictionary['from']['rev_dictionary']\n",
    "\n",
    "dictionary_to = dictionary['to']['dictionary']\n",
    "rev_dictionary_to = dictionary['to']['rev_dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rachel Pike : The science behind a climate headline EOS'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(train_X)):\n",
    "    train_X[i] += ' EOS'\n",
    "    \n",
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I speak in <NUM> minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four - year - old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than <NUM> years ago , bonds that took hold in the life of that small girl and never let go - - that small girl now living in San Francisco and speaking to you today ? EOS'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(test_X)):\n",
    "    test_X[i] += ' EOS'\n",
    "    \n",
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_second_dim(x, desired_size):\n",
    "    padding = tf.tile([[[0.0]]], tf.stack([tf.shape(x)[0], desired_size - tf.shape(x)[1], tf.shape(x)[2]], 0))\n",
    "    return tf.concat([x, padding], 1)\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size,\n",
    "                 from_dict_size, to_dict_size, learning_rate, batch_size):\n",
    "        \n",
    "        def cells(reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
    "        \n",
    "        def attention(encoder_out, seq_len, reuse=False):\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units = size_layer, \n",
    "                                                                    memory = encoder_out,\n",
    "                                                                    memory_sequence_length = seq_len)\n",
    "            return tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([cells(reuse) for _ in range(num_layers)]), \n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = size_layer)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "        \n",
    "        encoder_embedding = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
    "        decoder_embedding = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
    "        \n",
    "        encoder_out, encoder_state = tf.nn.dynamic_rnn(\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)]), \n",
    "            inputs = tf.nn.embedding_lookup(encoder_embedding, self.X),\n",
    "            sequence_length = self.X_seq_len,\n",
    "            dtype = tf.float32)\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        dense = tf.layers.Dense(to_dict_size)\n",
    "        decoder_cells = attention(encoder_out, self.X_seq_len)\n",
    "        \n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                inputs = tf.nn.embedding_lookup(decoder_embedding, decoder_input),\n",
    "                sequence_length = self.Y_seq_len,\n",
    "                time_major = False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell = decoder_cells,\n",
    "                helper = training_helper,\n",
    "                initial_state = decoder_cells.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\n",
    "                output_layer = dense)\n",
    "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = training_decoder,\n",
    "                impute_finished = True,\n",
    "                maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
    "        self.training_logits = training_decoder_output.rnn_output\n",
    "        \n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                embedding = decoder_embedding,\n",
    "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
    "                end_token = EOS)\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell = decoder_cells,\n",
    "                helper = predicting_helper,\n",
    "                initial_state = decoder_cells.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\n",
    "                output_layer = dense)\n",
    "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = predicting_decoder,\n",
    "                impute_finished = True,\n",
    "                maximum_iterations = 2 * tf.reduce_max(self.X_seq_len))\n",
    "        self.predicting_ids = predicting_decoder_output.sample_id\n",
    "        \n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        y_t = tf.argmax(self.training_logits,axis=2)\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.Y, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 512\n",
    "num_layers = 2\n",
    "embedded_size = 256\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 17:58:32.839742 140571602184000 deprecation.py:506] From /home/husein/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n",
      "W0903 17:58:32.883325 140571602184000 deprecation.py:323] From <ipython-input-10-d589d1c515de>:10: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0903 17:58:32.885724 140571602184000 deprecation.py:323] From <ipython-input-10-d589d1c515de>:31: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0903 17:58:32.890247 140571602184000 deprecation.py:323] From <ipython-input-10-d589d1c515de>:34: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0903 17:58:33.248548 140571602184000 deprecation.py:506] From /home/husein/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0903 17:58:34.044417 140571602184000 deprecation.py:323] From /home/husein/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0903 17:58:34.768882 140571602184000 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0903 17:58:34.788153 140571602184000 deprecation.py:506] From /home/husein/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Translator(size_layer, num_layers, embedded_size, len(dictionary_from), \n",
    "                len(dictionary_to), learning_rate,batch_size)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i.split():\n",
    "            ints.append(dic.get(k,UNK))\n",
    "        X.append(ints)\n",
    "    return X\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = str_idx(train_X, dictionary_from)\n",
    "test_X = str_idx(test_X, dictionary_from)\n",
    "train_Y = str_idx(train_Y, dictionary_to)\n",
    "test_Y = str_idx(test_Y, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:50<00:00,  1.60it/s, accuracy=0.22, cost=5.01] \n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.01it/s, accuracy=0.243, cost=4.56]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training avg loss 5.419281, training avg acc 0.162793\n",
      "epoch 1, testing avg loss 4.482214, testing avg acc 0.271014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:52<00:00,  1.60it/s, accuracy=0.339, cost=4.02]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.18it/s, accuracy=0.48, cost=3.27] \n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training avg loss 3.774772, training avg acc 0.370305\n",
      "epoch 2, testing avg loss 3.487123, testing avg acc 0.406801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:52<00:00,  1.60it/s, accuracy=0.382, cost=3.32]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.17it/s, accuracy=0.52, cost=2.9]  \n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training avg loss 3.030607, training avg acc 0.459418\n",
      "epoch 3, testing avg loss 3.090570, testing avg acc 0.458439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [11:09<00:00,  1.56it/s, accuracy=0.45, cost=2.64] \n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.17it/s, accuracy=0.548, cost=2.68]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training avg loss 2.602820, training avg acc 0.509087\n",
      "epoch 4, testing avg loss 2.911939, testing avg acc 0.481943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [11:08<00:00,  1.56it/s, accuracy=0.537, cost=2.11]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.12it/s, accuracy=0.554, cost=2.65]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training avg loss 2.315552, training avg acc 0.544459\n",
      "epoch 5, testing avg loss 2.882918, testing avg acc 0.484474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [11:09<00:00,  1.56it/s, accuracy=0.591, cost=1.75]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.16it/s, accuracy=0.525, cost=2.85]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training avg loss 2.103725, training avg acc 0.571946\n",
      "epoch 6, testing avg loss 2.934652, testing avg acc 0.476456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [11:09<00:00,  1.56it/s, accuracy=0.648, cost=1.49]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.18it/s, accuracy=0.525, cost=2.62]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training avg loss 1.938136, training avg acc 0.594995\n",
      "epoch 7, testing avg loss 2.857240, testing avg acc 0.491560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:51<00:00,  1.60it/s, accuracy=0.705, cost=1.25]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.21it/s, accuracy=0.525, cost=2.53]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training avg loss 1.796233, training avg acc 0.615908\n",
      "epoch 8, testing avg loss 2.869818, testing avg acc 0.493094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:53<00:00,  1.59it/s, accuracy=0.746, cost=1.06]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.16it/s, accuracy=0.497, cost=2.68]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training avg loss 1.671715, training avg acc 0.635134\n",
      "epoch 9, testing avg loss 2.953726, testing avg acc 0.484173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:59<00:00,  1.58it/s, accuracy=0.774, cost=0.913]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.13it/s, accuracy=0.52, cost=2.61] \n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training avg loss 1.562849, training avg acc 0.652766\n",
      "epoch 10, testing avg loss 3.039550, testing avg acc 0.482028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:52<00:00,  1.60it/s, accuracy=0.801, cost=0.867]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.09it/s, accuracy=0.48, cost=2.77] \n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, training avg loss 1.468104, training avg acc 0.668640\n",
      "epoch 11, testing avg loss 3.089940, testing avg acc 0.478801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:51<00:00,  1.60it/s, accuracy=0.842, cost=0.738]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.14it/s, accuracy=0.497, cost=2.79]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, training avg loss 1.381287, training avg acc 0.683440\n",
      "epoch 12, testing avg loss 3.186867, testing avg acc 0.470595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:52<00:00,  1.60it/s, accuracy=0.838, cost=0.687]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.15it/s, accuracy=0.486, cost=3.06]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, training avg loss 1.303354, training avg acc 0.697625\n",
      "epoch 13, testing avg loss 3.329625, testing avg acc 0.458667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:53<00:00,  1.60it/s, accuracy=0.854, cost=0.616]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.16it/s, accuracy=0.486, cost=3.15]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, training avg loss 1.226899, training avg acc 0.711621\n",
      "epoch 14, testing avg loss 3.403308, testing avg acc 0.458001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:52<00:00,  1.60it/s, accuracy=0.863, cost=0.503]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.13it/s, accuracy=0.497, cost=3.2] \n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, training avg loss 1.158215, training avg acc 0.724634\n",
      "epoch 15, testing avg loss 3.429555, testing avg acc 0.466972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:50<00:00,  1.60it/s, accuracy=0.89, cost=0.463] \n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.12it/s, accuracy=0.48, cost=3.11] \n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, training avg loss 1.092784, training avg acc 0.737181\n",
      "epoch 16, testing avg loss 3.440430, testing avg acc 0.470857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:49<00:00,  1.60it/s, accuracy=0.925, cost=0.364]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.16it/s, accuracy=0.486, cost=3.33]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, training avg loss 1.034970, training avg acc 0.748194\n",
      "epoch 17, testing avg loss 3.531271, testing avg acc 0.465216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:49<00:00,  1.60it/s, accuracy=0.931, cost=0.359]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.14it/s, accuracy=0.492, cost=3.46]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, training avg loss 0.984087, training avg acc 0.758383\n",
      "epoch 18, testing avg loss 3.630916, testing avg acc 0.460587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:50<00:00,  1.60it/s, accuracy=0.94, cost=0.291] \n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.12it/s, accuracy=0.469, cost=3.38]\n",
      "minibatch loop:   0%|          | 0/1042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, training avg loss 0.936458, training avg acc 0.767773\n",
      "epoch 19, testing avg loss 3.713086, testing avg acc 0.458853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 1042/1042 [10:49<00:00,  1.60it/s, accuracy=0.942, cost=0.266]\n",
      "minibatch loop: 100%|██████████| 23/23 [00:05<00:00,  4.11it/s, accuracy=0.458, cost=3.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, training avg loss 0.894548, training avg acc 0.775765\n",
      "epoch 20, testing avg loss 3.795089, testing avg acc 0.455244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for e in range(epoch):\n",
    "    pbar = tqdm.tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'minibatch loop')\n",
    "    train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, len(train_X))\n",
    "        maxlen = max([len(s) for s in train_X[i : index] + train_Y[i : index]])\n",
    "        batch_x, seq_x = pad_sentence_batch(train_X[i : index], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(train_Y[i : index], PAD)\n",
    "        feed = {model.X: batch_x,\n",
    "                model.Y: batch_y}\n",
    "        accuracy, loss, _ = sess.run([model.accuracy,model.cost,model.optimizer],\n",
    "                                    feed_dict = feed)\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(accuracy)\n",
    "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
    "    \n",
    "    \n",
    "    pbar = tqdm.tqdm(\n",
    "        range(0, len(test_X), batch_size), desc = 'minibatch loop')\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, len(test_X))\n",
    "        batch_x, seq_x = pad_sentence_batch(test_X[i : index], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(test_Y[i : index], PAD)\n",
    "        feed = {model.X: batch_x,\n",
    "                model.Y: batch_y,}\n",
    "        accuracy, loss = sess.run([model.accuracy,model.cost],\n",
    "                                    feed_dict = feed)\n",
    "\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(accuracy)\n",
    "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
    "    \n",
    "    print('epoch %d, training avg loss %f, training avg acc %f'%(e+1,\n",
    "                                                                 np.mean(train_loss),np.mean(train_acc)))\n",
    "    print('epoch %d, testing avg loss %f, testing avg acc %f'%(e+1,\n",
    "                                                              np.mean(test_loss),np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_dictionary_to = {int(k): v for k, v in rev_dictionary_to.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 198)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 20\n",
    "\n",
    "batch_x, seq_x = pad_sentence_batch(test_X[: test_size], PAD)\n",
    "batch_y, seq_y = pad_sentence_batch(test_Y[: test_size], PAD)\n",
    "feed = {model.X: batch_x}\n",
    "logits = sess.run(model.predicting_ids, feed_dict = feed)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 predict: Làm sao để nói bằng sự gắn bó giữa phụ nữ trong <NUM> thế hệ , về cách thức đáng ngạc nhiên của những cặp tôn giáo kia gắn giữ cơ thể trong cuộc sống <NUM> tuổi một lần với chị gái , mẹ và bà của cô trong ngày , ngày đêm và cả đêm ở Trung Quốc ngày càng có thêm giọng ưng , mẹ và mẹ trong một ngày bình thường ở Trung Quốc ngày càng tới hơn <NUM> năm về trước , gắn bó với các bạn trong ngày hôm nay ? \" Cô em bé giờ đang sinh sống trong đời sống của cô Francisco và nói với các bạn ngày hôm nay chưa ? Cứ mỗi ngày ? \" Cô bé giờ đang học trong một chiếc thuyền nhỏ của cô và không bao giờ để lại . . . . . . mà cô bé này đang sống ở San Francisco hay nói với các bạn ngày hôm nay chưa ? Cứ trong một cô bé bây giờ đã ngày qua đang nói chuyện tại San Francisco chưa từng có . . . . . . . . . . .\n",
      "0 actual: Làm sao tôi có thể trình bày trong <NUM> phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn <NUM> năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi - - cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n",
      "\n",
      "1 predict: Nó không phải một câu chuyện hoàn toàn . . . . . . . . . . . . . . . . . . này không phải một câu chuyện hoàn hảo . . . . . . . . . . . . . . . . . . . . . . . . này không phải một câu chuyện hoàn toàn . . . . . . . . . . . . . . . . . . . . . . . . . . . này không phải là một câu chuyện hoàn toàn . . . . . . . . . . . . . . . . . . . . . . . . này không phải là một câu chuyện hoàn toàn . . . . . . . . . . . . . . . . . . . . . . . . này không phải là một câu chuyện hoàn toàn . . . . . . . . . . . . . . . . . . . . . . . . này không phải là một câu\n",
      "1 actual: Câu chuyện này chưa kết thúc .\n",
      "\n",
      "2 predict: Một trò chơi ghép đầy châm Bear , được đặt cùng . . . . . . . . . . . . . . . . . . . . . . . . đó là một \" vườn \" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "2 actual: Nó là một trò chơi ghép hình vẫn đang được xếp .\n",
      "\n",
      "3 predict: Tôi muốn nói về một số đoạn nhỏ . . . . . . . . . . . . cho ta biết về một số mô hình . . . . . . . . . cho ta biết về một số mô hình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "3 actual: Hãy để tôi kể cho các bạn về vài mảnh ghép nhé .\n",
      "\n",
      "4 predict: Hãy tưởng tượng bản nhạc : Một người đàn ông đang đốt công việc của đời mình . Giải pháp này là của ông . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "4 actual: Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .\n",
      "\n",
      "5 predict: Ông là nhà thơ , nhà thơ viết , một nhà thơ , một người đàn ông đã bị hạn hi về hiến kết hợp của quốc gia và tự do của ông . . . . . . và tự do của ông . . . . . . . . . một kẻ sống sót qua hiệp ước của quốc gia và tự do của ông . . . . . . . . . . . . có tật về hiệp ước của quốc gia và tự do của ông . . . . . . và tự do của ông . . . . . . . . . . . . có tật về hiệp ước của quốc gia . . . . . . và tự do của ông . . . . . . . . . có ổn định . . . . . . một kẻ sáng suốt trong hiệp ước của quốc gia . . . . . . và tự do của ông . . . . . . . . . . . . . . . có ai đó bị hạn chế về thống nhất\n",
      "5 actual: Ông là nhà thơ , nhà viết kịch , một người mà cả cuộc đời chênh vênh trên tia hi vọng duy nhất rằng đất nước ông sẽ độc lập tự do .\n",
      "\n",
      "6 predict: Hãy thử hình dung là với một số khi các bạn có thể chuyển vào cái can đảm , đối nghịch với cuộc đời mình đã hoàn thành rác thải . \" Sự lãng phí \" . \" . . . hoàn chỉnh . \" . . . hoàn toàn mất đi . \" . . . hoàn toàn lãng phí . \" . . . hoàn toàn lãng phí . \" . . . hoàn toàn mất đi . \" . . . hoàn chỉnh . \" . . . hoàn toàn mất tích . . . . . . hoàn chỉnh . \" . . . hoàn toàn mất tích . . . . . . hoàn chỉnh . \" . . . hoàn toàn mất tích . . . . . . hoàn chỉnh . \" . . . hoàn toàn mất tích . . . . . . hoàn chỉnh . \" \" . . . hoàn chỉnh là rác thải hoàn chỉnh . \" . . . hoàn chỉnh hoàn toàn lãng phí . \" . . . hoàn chỉnh mất tích cực . \" . . . hoàn chỉnh hoàn toàn lãng\n",
      "6 actual: Hãy tưởng tượng ông , một người cộng sản tiến vào , đối diện sự thật rằng cả cuộc đời ông đã phí hoài .\n",
      "\n",
      "7 predict: Với những bạn từ ngữ dài , giờ đây bạn bè lại có anh ấy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7 actual: Ngôn từ , qua bao năm tháng là bạn đồng hành với ông , giờ quay ra chế giễu ông .\n",
      "\n",
      "8 predict: Anh ấy mở rộng trong sự im lặng . . . . . . . . . vào im lặng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vào im lặng . \" Anh ấy mở rộng trong sự im lặng . \" Anh ấy lọt vào sự im lặng . \" Anh ấy lọt vào sự im lặng . \" Anh ấy lọt vào sự im lặng . \" Anh ấy lọt vào sự im lặng . \" Anh ấy lọt vào sự im lặng . \"\n",
      "8 actual: Ông rút lui vào yên lặng .\n",
      "\n",
      "9 predict: Anh ấy mất bị mất lịch lịch lịch sử . © một cách tự nhiên . . . . . . . . . . . . . . . ông mất bị mất lịch sử . \" . . . . . . . . . . . . . . . . . . ông mất bị mất lịch sử . \" . . . . . . . . . . . . . . . ông mất bị mất lịch sử . \" . . . . . . . . . . . . . . . anh mất . \" Anh ấy chết . \" . . . anh mất . \" Anh ấy chết . \" . . . anh ấy mất bị mất lịch sử . \" . . . . . . . . . . . . anh mất . \" Anh ấy chết . \" . . . anh ấy mất bị chết mất . \" Anh ấy chết . \" . . . anh ấy mất . \" Anh ấy chết . \" . . . anh ấy mất . \" Anh ấy chết . \"\n",
      "9 actual: Ông qua đời , bị lịch sử quật ngã .\n",
      "\n",
      "10 predict: Ông tôi là người của tôi . . . . . . . . . là ông tôi . . . . . . là ông tôi . . . . . . là ông tôi . . . . . . là ông tôi . . . . . . là ông tôi . . . . . . là ông tôi . . . . . . là ông tôi . . . . . . . . . là ông tôi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . là ông tôi . . . . . . . . . là ông tôi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "10 actual: Ông là ông của tôi .\n",
      "\n",
      "11 predict: Tôi không bao giờ biết anh trong đời thực . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "11 actual: Tôi chưa bao giờ gặp ông ngoài đời .\n",
      "\n",
      "12 predict: Nhưng cuộc sống của chúng ta không còn báo nữa . . . . . . . . . . . . của chúng ta hơn cả kí ức . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \" \" . . . \" . . . \" \" . . . \" . . . \" . . . \" \" . . . \" . . . \" . . . \" \" . . . \" . . . \" . . . \" . . . \" \" . . . \" . . . \" . . . \" \" . . . \" . . . \" . . . \" \" . . . \" . . . \" . . . \" \" .\n",
      "12 actual: Nhưng cuộc đời ta nhiều hơn những gì ta lưu trong kí ức nhiều .\n",
      "\n",
      "13 predict: Bà ngoại tôi bao giờ để tôi trả lời mình . \" . . . \" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "13 actual: Bà tôi chưa bao giờ cho phép tôi quên cuộc đời của ông .\n",
      "\n",
      "14 predict: Nhiệm vụ của tôi không phải là cho nó là ở trong vô vọng , trong bài học của tôi là học hỏi , rằng , đúng vậy , lịch sử đã cố gắng nhấn mạnh với chúng ta , nhưng chúng ta lộn xộn . \" \" Đi . \" . . . \" và chúng ta rắc . \" chúng ta đi quá . \" \" Đi . \" \" Đi . \" . . . \" chúng ta \" . . . \" chúng ta \" . . . \" \" Đi . \" . . . \" chúng ta \" hay \" chúng ta chán ngấy với nhau . \" tắt . \" \" Đi . \" \" Đi . \" \" Đi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . được tìm cách móc buộc chúng ta , nhưng chúng tôi lộn xộn . \" \" Đi . \" \" Đi . \" \" Đi .\n",
      "14 actual: Nhiệm vụ của tôi là không để cuộc đời ấy qua trong vô vọng , và bài học của tôi là nhận ra rằng , vâng , lịch sử đã cố quật ngã chúng tôi , nhưng chúng tôi đã chịu đựng được .\n",
      "\n",
      "15 predict: Mẫu ghép ca của miếng ghép hình trong đầu xoay xoay theo bình phun , biển . . . . . . . . . . . . . . . . . . . . . là một con thuyền trong đầu . . . . . . . . . là một cái thuyền trong bình phun . \" . . . . . . trong một bình phun nhỏ của bình dữ . \" Nó là một màu của biển bẹp xoay . \" . . . . . . . . . . . . . . . . . trong một con dã quan đầu thôi . \" . . . . . . . . . là một màu trong quá trình bình thường . \" . . . . . . . . . . . . là một màu trong đầu . \" . . . . . . . . . . . . là một màu trong đầu . \" . . . . . . . . . . . là một màu trong quá trình bình sơ của mình . \" . . . . .\n",
      "15 actual: Mảnh ghép tiếp theo của tấm hình là một con thuyền trong sớm hoàng hôn lặng lẽ trườn ra biển .\n",
      "\n",
      "16 predict: Mẹ tôi , <NUM> khi bố cô ấy đã chết - trong một cái hôn , vừa trải qua hai con gái nhỏ . Cô bé có <NUM> bé gái . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . trong một cái cô bé . \" Cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô gái nhỏ . \" . . . <NUM> cô\n",
      "16 actual: Mẹ tôi , Mai , mới <NUM> tuổi khi ba của mẹ mất - - đã lập gia đình , một cuộc hôn nhân sắp đặt trước , đã có hai cô con gái nhỏ .\n",
      "\n",
      "17 predict: Trong đời mình , cuộc sống đã cất hạng ra một nhiệm vụ : Sự trốn tránh đi từ và một cuộc sống mới ở Úc . . . . . . ở Úc và một cuộc sống mới ở Úc . . . . . . . . . . . . mới ở Úc . \" . . . một đời sống mới ở Úc . . . . . . . . . mới ở Úc . \" . . . một đời sống mới ở Úc . . . . . . . . . mới ở Úc . \" . . . một đời sống mới ở Úc . . . . . . . . . mới ở Úc . \" . . . một đời sống mới ở Úc . . . . . . . . . mới ở Úc . \" . . . một đời mới ở Úc . \" . . . . . . mới ở Úc . \" . . . còn lại ở Úc . \" . . . . . . mới ở Úc . \" . . . còn lại ở Úc\n",
      "17 actual: Với mẹ , cuộc đời cô đọng vào nhiệm vụ duy nhất : để gia đình mẹ trốn thoát và bắt đầu cuộc sống mới ở Úc .\n",
      "\n",
      "18 predict: Cô ấy rất chắc chắn rằng mình không thành công . \" thành công \" . . . . . . . . . . . . . . . . . . . . . . . . nó hiển thị thành công . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "18 actual: Mẹ không bao giờ chấp nhận được là mẹ sẽ không thành công .\n",
      "\n",
      "19 predict: Sau tủ lạnh <NUM> năm nghiên cứu viễn tưởng , một cái thuyền đã lọt như một tàu lớn . \" Việc đánh bắt cá voi . \" . . . như một tàu đánh bắt cá . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . chỉ để vắt khơi . \" một tàu đánh bắt làm mồi . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . chỉ để vắt khơi . \" một tàu đánh bắt làm mồi . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . \" . . . chỉ để đại dương . \" một tàu đánh bắt làm mồi . \" . . . \" . . . \" . . . \" . . . \"\n",
      "19 actual: Thế là sau bốn năm , một trường thiên đằng đẵng hơn cả trong truyện , một chiếc thuyền trườn ra biển nguỵ trang là thuyền đánh cá .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rejected = ['PAD', 'EOS', 'UNK', 'GO']\n",
    "\n",
    "for i in range(test_size):\n",
    "    predict = [rev_dictionary_to[i] for i in logits[i] if rev_dictionary_to[i] not in rejected]\n",
    "    actual = [rev_dictionary_to[i] for i in batch_y[i] if rev_dictionary_to[i] not in rejected]\n",
    "    print(i, 'predict:', ' '.join(predict))\n",
    "    print(i, 'actual:', ' '.join(actual))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
